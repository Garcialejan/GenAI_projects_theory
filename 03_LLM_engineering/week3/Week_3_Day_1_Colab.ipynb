{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTygxy-RAn1f"
      },
      "source": [
        "# Week 3 Day 1\n",
        "\n",
        "Welcome to Google Colab - using free and powerful compute in the cloud.\n",
        "\n",
        "Google Colab gives you a remote Notebook-style browser window on to a machine, and you run code \"locally\" on that machine.\n",
        "\n",
        "## Benefits of Colab\n",
        "\n",
        "1. Free access to T4 GPUs!\n",
        "2. Easy ability to share code and collaborate on it\n",
        "3. Everyone gets to use identical code - no environment differences\n",
        "\n",
        "## Downsides of Colab\n",
        "\n",
        "1. As it's free, Google reserves the right to bump you off the box at any point (\"reset the runtime\"), and this happens most quickly if no code is running. They can also downgrade you from a T4 to a CPU-only box. Sometimes this happens silently. You need to start everything again from the top. Paid plans last longer.\n",
        "2. You need to pip install packages every single time (but no pip installs today)\n",
        "3. There's some latency - it's not as interactive as coding on your own box\n",
        "\n",
        "## Survival Guide\n",
        "\n",
        "1. Always start by pressing the drop down arrow by Connect on the top right, and \"Connect to a hosted runtime: T4\"\n",
        "2. From that dropdown, \"View Resources\" to check you have a GPU and monitor memory\n",
        "3. If things go awry, Runtime >> Disconnect and Delete Runtime, Connect again, and then run cells from the top\n",
        "4. Always run the pip installs! Ignore pip dependency errors.\n",
        "\n",
        "Runtime >> Restart session: this restarts the Python Kernel, but pip packages remain installed and the disk remains the same.\n",
        "\n",
        "Runtime >> Diconnect and delete session. This wipes everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iQqYgGVYnhco"
      },
      "outputs": [],
      "source": [
        "# !uv pip install -q --upgrade transformers==4.56.2\n",
        "# !uv pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â¿CUDA disponible? True\n",
            "Dispositivo actual: 0\n",
            "Nombre de la GPU: NVIDIA GeForce RTX 4060 Ti\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Â¿CUDA disponible?\", torch.cuda.is_available())\n",
        "print(\"Dispositivo actual:\", torch.cuda.current_device())\n",
        "print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E2aO6PbB0WU3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Nov 26 12:43:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 575.64.01              Driver Version: 576.88         CUDA Version: 12.9     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4060 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
            "|  0%   43C    P5             27W /  165W |    1454MiB /  16380MiB |     10%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "NOT CONNECTED TO A T4\n"
          ]
        }
      ],
      "source": [
        "# Let's check the GPU - it should be a Tesla T4 if yyou are working with colab\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('Tesla T4') >= 0:\n",
        "    print(\"Success - Connected to a T4\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO A T4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV8_hr1rCGUr"
      },
      "source": [
        "# Connecting Hugging Face\n",
        "\n",
        "You'll need to log in to the HuggingFace hub if you've not done so before.\n",
        "\n",
        "1. If you haven't already done so, create a **free** HuggingFace account at https://huggingface.co and navigate to Settings from the user menu on the top right. Then Create a new API token, giving yourself write permissions.  \n",
        "\n",
        "**IMPORTANT** when you create your HuggingFace API key, please be sure to select WRITE permissions for your key by clicking on the WRITE tab, otherwise you may get problems later. Not \"fine-grained\" but \"write\".\n",
        "\n",
        "2. Back here in colab, press the \"key\" icon on the side panel to the left, and add a new secret:  \n",
        "  In the name field put `HF_TOKEN`  \n",
        "  In the value field put your actual token: `hf_...`  \n",
        "  Ensure the notebook access switch is turned ON.\n",
        "\n",
        "3. Execute the cell below to log in. You'll need to do this on each of your colabs. It's a really useful way to manage your secrets without needing to type them into colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZR-wgFH-CKtO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# from google.colab import userdata\n",
        "load_dotenv(override=True)\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "# hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jTthxWyAJJZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", dtype=torch.float16, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")\n",
        "prompt = \"A class of students learning AI engineering in a vibrant pop-art style\"\n",
        "image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HAeRMxVdJMDF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Restart the kernel\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UubQ06ZvEOj-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "pipe = DiffusionPipeline.from_pretrained(model_name,\n",
        "                                         dtype=torch.float16,\n",
        "                                         use_safetensors=True,\n",
        "                                         variant=\"fp16\")\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"A class of data scientists learning AI engineering in a vibrant high-energy pop-art style\"\n",
        "\n",
        "image = pipe(prompt=prompt, num_inference_steps=30).images[0]\n",
        "\n",
        "display(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeE9YBDoXyvi"
      },
      "outputs": [],
      "source": [
        "# Restart the kernel\n",
        "\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmijo03lNsCC"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "base = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                                         dtype=torch.float16,\n",
        "                                         variant=\"fp16\",\n",
        "                                         use_safetensors=True)\n",
        "base.to(\"cuda\")\n",
        "refiner = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "                                            text_encoder_2=base.text_encoder_2,\n",
        "                                            vae=base.vae,\n",
        "                                            dtype=torch.float16,\n",
        "                                            use_safetensors=True,\n",
        "                                            variant=\"fp16\",)\n",
        "refiner.to(\"cuda\")\n",
        "\n",
        "# Define how many steps and what % of steps to be run on each experts (80/20) here\n",
        "n_steps = 40\n",
        "high_noise_frac = 0.8\n",
        "\n",
        "prompt = \"A class of data scientists learning AI engineering in a vibrant high-energy pop-art style\"\n",
        "\n",
        "# run both experts\n",
        "image = base(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=n_steps,\n",
        "    denoising_end=high_noise_frac,\n",
        "    output_type=\"latent\",\n",
        ").images\n",
        "\n",
        "image = refiner(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=n_steps,\n",
        "    denoising_start=high_noise_frac,\n",
        "    image=image,\n",
        ").images[0]\n",
        "\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este cÃ³digo implementa **la inferencia en dos etapas del modelo Stable Diffusion XL (SDXL)** usando el **pipeline base + refiner**, una estrategia optimizada para generar imÃ¡genes de alta calidad con menor costo computacional.\n",
        "\n",
        "Vamos a desglosarlo paso a paso:\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”§ **1. Cargar el pipeline base**\n",
        "```python\n",
        "base = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                                         torch_dtype=torch.float16,\n",
        "                                         variant=\"fp16\",\n",
        "                                         use_safetensors=True)\n",
        "base.to(\"cuda\")\n",
        "```\n",
        "- Carga el **modelo base de SDXL**, que es el encargado de generar la imagen desde ruido.\n",
        "- Usa **precisiÃ³n mixta** (`float16`) para ahorrar memoria y acelerar la inferencia.\n",
        "- `variant=\"fp16\"` indica que se descarguen los pesos en formato FP16 (mÃ¡s ligero).\n",
        "- `use_safetensors=True` usa el formato seguro y rÃ¡pido `safetensors` en lugar de `pickle`.\n",
        "- Mueve el modelo a la GPU con `.to(\"cuda\")`.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”§ **2. Cargar el pipeline refiner**\n",
        "```python\n",
        "refiner = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "                                            text_encoder_2=base.text_encoder_2,\n",
        "                                            vae=base.vae,\n",
        "                                            torch_dtype=torch.float16,\n",
        "                                            use_safetensors=True,\n",
        "                                            variant=\"fp16\")\n",
        "refiner.to(\"cuda\")\n",
        "```\n",
        "- Carga el **modelo refinador de SDXL**, que no genera desde cero, sino que **mejora una imagen parcial ya generada**.\n",
        "- **Reutiliza componentes del modelo base**:\n",
        "  - `text_encoder_2`: SDXL usa dos text encoders; el segundo ya estÃ¡ cargado en `base`, asÃ­ que lo comparte (ahorra RAM y evita duplicados).\n",
        "  - `vae`: el autoencoder variacional (que decodifica latentes a pÃ­xeles) tambiÃ©n se comparte.\n",
        "- Esto es clave: el refiner **no necesita su propio VAE ni su propio text encoder 2**, ya que opera sobre la misma representaciÃ³n latente y semÃ¡ntica que el base.\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ **3. Configurar la estrategia de inferencia en dos etapas**\n",
        "```python\n",
        "n_steps = 40\n",
        "high_noise_frac = 0.8\n",
        "```\n",
        "- **40 pasos de denoising en total**.\n",
        "- **El 80% del proceso (32 pasos)** lo hace el **modelo base**.\n",
        "- **El 20% final (8 pasos)** lo hace el **refiner**.\n",
        "\n",
        "Esto se basa en la observaciÃ³n de que:\n",
        "> Los primeros pasos de denoising definen la composiciÃ³n general, mientras que los Ãºltimos refinan detalles finos (texturas, bordes, coherencia local).  \n",
        "> El **refiner estÃ¡ entrenado especÃ­ficamente para mejorar esos Ãºltimos pasos**, y no necesita ver todo el ruido inicial.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ–¼ï¸ **4. Primera etapa: generaciÃ³n parcial con el modelo base**\n",
        "```python\n",
        "image = base(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=n_steps,\n",
        "    denoising_end=high_noise_frac,    # â† Detiene en el 80%\n",
        "    output_type=\"latent\",             # â† Devuelve latentes, no pÃ­xeles\n",
        ").images\n",
        "```\n",
        "- Genera una imagen, pero **solo hasta el 80% del proceso**.\n",
        "- `output_type=\"latent\"`: devuelve la representaciÃ³n **latente** (un tensor de baja resoluciÃ³n en el espacio del VAE), **no una imagen en pÃ­xeles**.\n",
        "- Esto es eficiente: no se decodifica a RGB innecesariamente.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ–¼ï¸ **5. Segunda etapa: refinamiento con el modelo refiner**\n",
        "```python\n",
        "image = refiner(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=n_steps,\n",
        "    denoising_start=high_noise_frac,  # â† Empieza donde terminÃ³ el base\n",
        "    image=image,                      # â† Recibe los latentes del base\n",
        ").images[0]\n",
        "```\n",
        "- El refiner **continÃºa el denoising desde el 80% hasta el 100%**.\n",
        "- Recibe los **latentes intermedios** del base y los mejora.\n",
        "- Finalmente, **decodifica a una imagen RGB** (porque `output_type` no se especifica, usa el valor predeterminado: `\"pil\"`).\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… **Â¿Por quÃ© hacer esto?**\n",
        "- **Mayor calidad visual**: el refiner mejora coherencia, texturas y detalles finos.\n",
        "- **Eficiencia**: el refiner no necesita procesar los pasos iniciales (mÃ¡s costosos en tÃ©rminos de estructura global).\n",
        "- **Ahorro de memoria**: al compartir `text_encoder_2` y `vae`, se evita cargar duplicados.\n",
        "- **SDXL fue diseÃ±ado asÃ­**: la arquitectura oficial de Stability AI usa esta estrategia en producciÃ³n.\n",
        "\n",
        "---\n",
        "\n",
        "### âš ï¸ Notas importantes\n",
        "- **Ambos modelos deben estar en la GPU** (`.to(\"cuda\")`).\n",
        "- **El prompt debe ser el mismo en ambas etapas** para coherencia.\n",
        "- Puedes ajustar `high_noise_frac`:\n",
        "  - Valores mÃ¡s altos (ej. `0.8â€“0.9`) â†’ mÃ¡s trabajo para el base â†’ mÃ¡s rÃ¡pido pero menos refinamiento.\n",
        "  - Valores mÃ¡s bajos (ej. `0.6`) â†’ mÃ¡s refinamiento, pero el refiner recibe una imagen menos definida.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Œ En resumen:\n",
        "Este cÃ³digo **genera una imagen con SDXL usando dos modelos especializados**:  \n",
        "1. **Base**: crea la imagen desde el ruido hasta un estado intermedio.  \n",
        "2. **Refiner**: toma ese estado intermedio y lo mejora en los pasos finales.  \n",
        "\n",
        "Es la forma **recomendada y Ã³ptima** de usar Stable Diffusion XL para alta calidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEQ2eeW7Kaxu"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fYqEcdsPxmBh"
      },
      "outputs": [],
      "source": [
        "# !uv pip install --upgrade datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5G9d2XLNo1Z"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from IPython.display import Audio\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\",\n",
        "                       \"microsoft/speecht5_tts\",\n",
        "                       device='cuda')\n",
        "\n",
        "embeddings_dataset = load_dataset(\"matthijs/cmu-arctic-xvectors\", split=\"validation\", trust_remote_code=True)\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "speech = synthesiser(\"Hi to an artificial intelligence engineer, on the way to mastery!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "Audio(speech[\"audio\"], rate=speech[\"sampling_rate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vk0La6POTcT"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv1Iv0vjPlI5"
      },
      "source": [
        "## This next cells will only work on a powerful GPU box like an A100\n",
        "\n",
        "This is not available on a free T4 box.\n",
        "\n",
        "I just want to show off what's possible with a small paid budget..\n",
        "\n",
        "Rough pricing:\n",
        "\n",
        "- $9.99 = 100 compute units\n",
        "- An A100 = 5.37 compute units per hour as of Oct 2025 (for me)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYoVlIwb5YEL"
      },
      "outputs": [],
      "source": [
        "# Let's check the GPU - it should be an A100\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('A100') >= 0:\n",
        "    print(\"Success - Connected to an NVIDIA A100\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO AN A100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw_WV7SgTWBv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "from IPython.display import display\n",
        "from datetime import datetime\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
        "prompt = \"A class of data scientists learning AI engineering in a vibrant high-energy pop-art style\"\n",
        "\n",
        "# Generate the image using the GPU\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    guidance_scale=0.0,\n",
        "    num_inference_steps=4,\n",
        "    max_sequence_length=256,\n",
        "    generator=generator\n",
        ").images[0]\n",
        "\n",
        "display(image)\n",
        "\n",
        "stop = datetime.now()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE7Fcljg56x1"
      },
      "outputs": [],
      "source": [
        "# Cost estimate\n",
        "\n",
        "seconds = (stop-start).total_seconds()\n",
        "units_per_hour = 5.37\n",
        "estimated_units = (5.37 / 3600) * seconds\n",
        "estimated_cost = estimated_units * (9.99/100)\n",
        "print(f\"This took {seconds:.1f} seconds and cost an estimated ${estimated_cost:.3f}\")\n",
        "\n",
        "# But there's a catch - you pay for all the time the kernel is active, not just while it's actually calculating!\n",
        "# So remember to shut down a Paid kernel.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ysVs1HD0yyD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm-engineering",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
